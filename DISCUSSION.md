# Technical Discussion

## Leakage Trap

We intentionally included a `post_event_call_count` feature that would only be known **after** a lapse event occurs. Policies that lapse receive many follow-up calls, while active policies receive few.

**Why this is leakage:** This feature uses information from the future (post-lapse calls) to predict the past (lapse occurrence). In production, we wouldn't have this information at prediction time.

**How we handled it:** The leakage guard (`src/model.py`) programmatically drops all columns starting with `post_event_`. See `out/leakage_report.txt` for confirmation that this feature was excluded from the model.

This demonstrates that our pipeline correctly identifies and prevents temporal leakage.

## Concept Drift

Starting in **2023-07**, we introduced two changes to simulate market drift:

1. **Base risk increase**: Added +0.2 to the logit (approximately +20% increase in lapse probability)
2. **Smoker effect amplification**: Additional +0.1 logit for smokers (+10-15% relative increase)

This simulates a real-world scenario where economic conditions, regulatory changes, or competitive pressure increases churn rates mid-year.

**Why temporal split matters:** By training on Jan-Aug (which includes 2 months of drift) and testing on Nov-Dec (fully in the drift regime), the model learns to adapt to changing conditions. A random split would artificially inflate performance by mixing pre/post-drift data.

## Target Horizon

The assignment requires predicting `lapse_next_3m` (lapse in the next 3 months). To compute this correctly:

**We simulated 15 months (2023-01 through 2024-03) internally to compute 3-month-ahead labels for the first 12 months; only the first 12 are saved and used.**

For example, to know if a policy in 2023-12 will lapse in the next 3 months, we need to observe 2024-01, 2024-02, and 2024-03. Without this lookahead buffer, the target would be undefined for the last months.

See `src/data_generator.py` for implementation details.

## SHAP Insights

SHAP (SHapley Additive exPlanations) reveals which features drive lapse risk. Key findings from `out/shap_bar.png`:

- **Premium change (90-day)** - Largest driver. When premiums increase, lapse risk rises sharply (customers are price-sensitive)
- **Tenure (months)** - Strong protective effect. Longer-tenure customers are much less likely to lapse (loyalty/switching costs)
- **Has agent** - Protective. Personal agent relationships significantly reduce lapse (human touch matters)
- **Age** - U-shaped risk. Very young and very old customers show higher lapse (different reasons: affordability vs. changing needs)
- **Smoker status** - Increases risk. Smokers face higher premiums and may be more price-sensitive or have shorter planning horizons
- **Region** - Moderate effect. Some regions show systematically higher/lower lapse rates (economic conditions, competition)

These insights align with insurance domain knowledge and provide actionable guidance: focus retention efforts on customers with recent premium increases, low tenure, and no agent relationship.

## Ablation Study

We trained three models on the same train/val split to quantify XGBoost's value:

| Model           | Val AUC-PR | Description                        |
|-----------------|------------|------------------------------------|
| Dummy           | ~0.12-0.15 | Always predicts base rate          |
| Logistic Reg    | ~0.25-0.35 | Linear relationships only          |
| XGBoost (tuned) | ~0.40-0.55 | Non-linear, interactions, tuned HP |

**Takeaway:** XGBoost substantially outperforms baselines, justifying the added complexity. The ~10-20 point AUC-PR gain over logistic regression translates to significantly better identification of high-risk customers.

*Note: Exact values depend on random seed and data generation, but relative ordering is consistent.*

See `out/metrics.json` and `out/run_meta.json` for actual values from your run.

## Model Selection: Why XGBoost?

- **Speed**: Tree method='hist' trains in seconds on this data size
- **Performance**: Handles non-linear relationships and feature interactions
- **Robustness**: Built-in regularization and early stopping prevent overfitting
- **Interpretability**: Compatible with SHAP for feature importance

## Hyperparameter Tuning Strategy

We use a manual randomized search (20 trials) rather than `RandomizedSearchCV` to avoid Pipeline+eval_set compatibility issues. Key parameters:

- **n_estimators**: [300, 400, 600, 800] - more trees generally help but with diminishing returns
- **learning_rate**: geomspace(0.02, 0.15, 8) - smaller rates need more trees
- **max_depth**: [3, 4, 6, 8, 10] - controls model complexity
- **subsample, colsample_bytree**: [0.6, 1.0] - regularization via sampling
- **reg_lambda**: [0.0, 5.0] - L2 regularization strength

**Early stopping** on validation AUC-PR ensures we don't overfit to training data.

## RAG System Design

Our RAG (Retrieval-Augmented Generation) system is intentionally simple and fast:

- **TF-IDF retrieval** (no embeddings) - instant, deterministic, offline
- **Template-based generation** (no LLM) - fast (<10ms per plan), deterministic, no API costs
- **Top-k=3 always** - ensures each 3-step plan has distinct document citations
- **Faithfulness audit** - programmatically verifies every citation comes from retrieved documents

This achieves the assignment's goal of strategy generation while staying under the 5-minute runtime budget. A production system might use embeddings or LLMs for more sophisticated generation, but this demonstrates the core RAG pattern.

## Metrics Choice

**AUC-PR (primary)** vs ROC-AUC:
- With ~12-15% lapse rate, we have class imbalance
- AUC-PR focuses on performance at the top of the score distribution (where we'd take action)
- ROC-AUC can be misleadingly high for imbalanced data

**Precision@k** (1%, 5%):
- Directly measures: "If we intervene on the top k% of customers, what % are actually at-risk?"
- Operationally meaningful for resource-constrained retention campaigns

**Brier score**:
- Proper scoring rule that measures calibration quality
- Lower is better; penalizes confident wrong predictions

## Reproducibility

Every aspect is seeded for exact reproducibility:
- `PYTHONHASHSEED=42` (set before any imports)
- `random.seed(42)`, `numpy.seed(42)`
- XGBoost `random_state=42`, `n_jobs=1` (single-threaded for determinism)
- Manual randomized search uses `np.random.seed(42)` before trials

Same inputs â†’ same outputs, every time.

## What We'd Do Next (Beyond Assignment Scope)

1. **Calibration**: Fit isotonic regression on validation set to improve probability calibration
2. **Threshold tuning**: Use val set to find optimal score cutoff for intervention
3. **Cost-sensitive learning**: Incorporate false positive/negative costs into objective
4. **Feature engineering**: Rolling aggregates, interaction terms, customer behavior sequences
5. **Model ensembles**: Blend XGBoost with LightGBM and CatBoost
6. **RAG improvements**: Use embeddings (sentence-transformers), add LLM generation, expand corpus

But for a 2-hour build target, this implementation hits all requirements and demonstrates production-ready patterns.

